# ==========================================
# Local LLM Research Analytics Tool
# Environment Configuration Template
# ==========================================
# Copy this file to .env and fill in your values
# NEVER commit .env to version control
# ==========================================

# ------------------------------------------
# LLM Provider Selection
# ------------------------------------------
# Choose your local LLM provider: "ollama" or "foundry_local"
LLM_PROVIDER=ollama

# ------------------------------------------
# Ollama Configuration
# ------------------------------------------
# Host URL for Ollama server
OLLAMA_HOST=http://localhost:11434

# Model to use for inference (must support tool calling)
# Recommended: qwen2.5:7b-instruct, llama3.1:8b, mistral:7b-instruct
OLLAMA_MODEL=qwen2.5:7b-instruct

# ------------------------------------------
# Microsoft Foundry Local Configuration
# ------------------------------------------
# API endpoint for Foundry Local (default port: 55588)
FOUNDRY_ENDPOINT=http://127.0.0.1:55588

# Model alias to use (e.g., phi-4, qwen2.5-0.5b, mistral-7b)
FOUNDRY_MODEL=phi-4

# Auto-start Foundry Local using SDK (requires foundry-local-sdk)
# Set to "true" to automatically start the model on agent initialization
FOUNDRY_AUTO_START=false

# ------------------------------------------
# SQL Server Configuration
# ------------------------------------------
# SQL Server hostname or IP address
SQL_SERVER_HOST=localhost

# SQL Server port (default: 1433)
SQL_SERVER_PORT=1433

# Database name to connect to
# For Docker setup, use: ResearchAnalytics
SQL_DATABASE_NAME=ResearchAnalytics

# Trust the server certificate (required for self-signed certs)
SQL_TRUST_SERVER_CERTIFICATE=true

# ------------------------------------------
# SQL Server Authentication
# ------------------------------------------
# For Docker setup (default credentials):
#   SQL_USERNAME=sa
#   SQL_PASSWORD=LocalLLM@2024!
#
# For Windows Authentication, leave both blank
# For SQL Server Authentication, provide username and password

# SQL Server username
SQL_USERNAME=sa

# SQL Server password
SQL_PASSWORD=LocalLLM@2024!

# ------------------------------------------
# Docker SQL Server Configuration
# ------------------------------------------
# Password for SQL Server SA account in Docker
# IMPORTANT: Change this for production!
MSSQL_SA_PASSWORD=LocalLLM@2024!

# ------------------------------------------
# MSSQL MCP Server Configuration
# ------------------------------------------
# Path to the MSSQL MCP Server index.js file
# After cloning and building: SQL-AI-samples/MssqlMcp/Node/dist/index.js
#
# Windows example:
# MCP_MSSQL_PATH=C:\Projects\SQL-AI-samples\MssqlMcp\Node\dist\index.js
#
# Linux/Mac example:
# MCP_MSSQL_PATH=/home/user/SQL-AI-samples/MssqlMcp/Node/dist/index.js

MCP_MSSQL_PATH=/path/to/SQL-AI-samples/MssqlMcp/Node/dist/index.js

# Set to "true" for read-only mode (safer for exploration)
MCP_MSSQL_READONLY=false

# ------------------------------------------
# Application Configuration
# ------------------------------------------
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Streamlit server port
STREAMLIT_PORT=8501

# ------------------------------------------
# Response Caching
# ------------------------------------------
# Enable caching of LLM responses for repeated queries
CACHE_ENABLED=true

# Maximum number of cached responses
CACHE_MAX_SIZE=100

# Cache time-to-live in seconds (0 = no expiration)
# Default: 3600 (1 hour)
CACHE_TTL_SECONDS=3600

# ------------------------------------------
# Rate Limiting
# ------------------------------------------
# Enable rate limiting for LLM API calls
RATE_LIMIT_ENABLED=false

# Maximum requests per minute
RATE_LIMIT_RPM=60

# Maximum burst size (requests allowed before throttling)
RATE_LIMIT_BURST=10

# ------------------------------------------
# Development Settings
# ------------------------------------------
# Enable debug mode (more verbose output)
DEBUG=false

# Enable MCP server debug logging
MCP_DEBUG=false
