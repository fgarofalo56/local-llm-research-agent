# üîç Local LLM Research Analytics Tool

> **A 100% local smart chat agent for SQL Server data analytics. Query your database using natural language with complete privacy - all inference runs locally via Ollama or Microsoft Foundry Local.**

---

## ‚ú® Features

| Feature | Status | Description |
|---------|--------|-------------|
| üîí **Fully Local** | ‚úÖ | No cloud APIs - all processing on your machine |
| üí¨ **Natural Language SQL** | ‚úÖ | Ask questions about your data in plain English |
| üîå **MCP Integration** | ‚úÖ | Extensible tool architecture via Model Context Protocol |
| ‚å®Ô∏è **CLI Interface** | ‚úÖ | Command-line chat for development |
| üåê **Streamlit Web UI** | ‚úÖ | User-friendly web interface |
| üîê **Privacy First** | ‚úÖ | Your data never leaves your network |
| üóÉÔ∏è **Sample Database** | ‚úÖ | Docker-based SQL Server with demo data |
| ü¶ô **Multiple LLM Providers** | ‚úÖ | Ollama or Microsoft Foundry Local |
| ‚ö° **Streaming Responses** | ‚úÖ | Real-time token streaming |

---

## üìë Table of Contents

- [Quick Start](#-quick-start)
- [Docker Setup](#-docker-setup-sql-server-with-sample-data)
- [MSSQL MCP Server Setup](#-mssql-mcp-server-setup)
- [Configuration](#Ô∏è-configuration)
- [Running the Agent](#-running-the-agent)
- [Testing the Agent](#-testing-the-agent)
- [MCP Tools Reference](#-mcp-tools-reference)
- [Architecture](#Ô∏è-architecture)
- [Development](#-development)
- [Contributing](#-contributing)
- [License](#-license)

---

## üöÄ Quick Start

### üì¶ Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| Python | 3.11+ | Required |
| [Ollama](https://ollama.com/) | Latest | LLM inference (or Foundry Local) |
| [Docker Desktop](https://www.docker.com/products/docker-desktop/) | Latest | For SQL Server |
| Node.js | 18+ | For MSSQL MCP Server |
| Git | Latest | Required |

### üì¶ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/local-llm-research-agent.git
cd local-llm-research-agent

# Install Python dependencies (using uv - recommended)
uv sync

# Or using pip
pip install -r requirements.txt

# Copy environment template
cp .env.example .env
```

### ü¶ô Pull the Ollama Model

```bash
# Recommended model for tool calling
ollama pull qwen2.5:7b-instruct

# Alternative models
ollama pull llama3.1:8b
ollama pull mistral:7b-instruct
```

> üí° **Tip**: `qwen2.5:7b-instruct` provides the best balance of performance and tool-calling capability.

---

## üê≥ Docker Setup (SQL Server with Sample Data)

The project includes a complete Docker setup with SQL Server 2022 and a pre-populated research analytics database.

### üóÑÔ∏è Database Overview

The sample database (`ResearchAnalytics`) contains:

| Table | Description | Records |
|-------|-------------|---------|
| Departments | Research departments (AI, ML, NLP, etc.) | 8 |
| Researchers | Team members with roles and salaries | 23 |
| Projects | Research projects with budgets and status | 15 |
| Publications | Academic papers and reports | 10 |
| Datasets | Research datasets with metadata | 10 |
| Experiments | ML experiments with results | 11 |
| Funding | Grants and funding sources | 12 |
| Equipment | Lab equipment and resources | 10 |

Plus 3 useful views: `vw_ActiveProjects`, `vw_ResearcherPublications`, `vw_ProjectFunding`

### üöÄ Starting the Database

#### Option 1: Quick Setup (Windows)

```bash
cd docker
setup-database.bat
```

#### Option 2: Manual Setup (All Platforms)

```bash
cd docker

# Start SQL Server container
docker compose up -d mssql

# Wait for SQL Server to be healthy
docker compose ps

# Run initialization scripts
docker compose --profile init up mssql-tools
```

### üîå Connection Details

| Setting | Value |
|---------|-------|
| **Server** | `localhost,1433` |
| **Database** | `ResearchAnalytics` |
| **Username** | `sa` |
| **Password** | `LocalLLM@2024!` (or your `MSSQL_SA_PASSWORD`) |

### üîß Testing the Connection

```bash
# Using sqlcmd (if installed)
sqlcmd -S localhost,1433 -U sa -P "LocalLLM@2024!" -d ResearchAnalytics -Q "SELECT COUNT(*) FROM Researchers"

# Using Docker exec
docker exec -it local-llm-mssql /opt/mssql-tools18/bin/sqlcmd \
  -S localhost -U sa -P "LocalLLM@2024!" -No \
  -Q "SELECT COUNT(*) AS ResearcherCount FROM ResearchAnalytics.dbo.Researchers"
```

### üìã Managing the Database

```bash
# View container logs
docker compose logs -f mssql

# Stop the database (preserves data)
docker compose down

# Stop and DELETE all data (fresh start)
docker compose down -v

# Restart with fresh data
docker compose down -v
docker compose up -d mssql
docker compose --profile init up mssql-tools
```

---

## üîå MSSQL MCP Server Setup

The MSSQL MCP Server provides tools for SQL Server interaction.

### üì¶ Installation

```bash
# Clone the MSSQL MCP Server repository
git clone https://github.com/Azure-Samples/SQL-AI-samples.git

# Navigate to Node.js implementation
cd SQL-AI-samples/MssqlMcp/Node

# Install dependencies
npm install

# Note the full path to dist/index.js
# Example: C:\Projects\SQL-AI-samples\MssqlMcp\Node\dist\index.js
```

### ‚öôÔ∏è Configure the Path

Update your `.env` file:

```bash
# Windows example
MCP_MSSQL_PATH=C:\Projects\SQL-AI-samples\MssqlMcp\Node\dist\index.js

# Linux/Mac example
MCP_MSSQL_PATH=/home/user/SQL-AI-samples/MssqlMcp/Node/dist/index.js
```

---

## ‚öôÔ∏è Configuration

### Environment Variables (.env)

```bash
cp .env.example .env
```

Configure for the Docker database:

```bash
# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Provider: "ollama" or "foundry_local"
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen2.5:7b-instruct

# Microsoft Foundry Local Configuration (alternative)
FOUNDRY_ENDPOINT=http://127.0.0.1:55588
FOUNDRY_MODEL=phi-4
FOUNDRY_AUTO_START=false

# =============================================================================
# SQL Server Configuration
# =============================================================================
SQL_SERVER_HOST=localhost
SQL_SERVER_PORT=1433
SQL_DATABASE_NAME=ResearchAnalytics
SQL_TRUST_SERVER_CERTIFICATE=true

# SQL Server Authentication
SQL_USERNAME=sa
SQL_PASSWORD=LocalLLM@2024!

# =============================================================================
# MCP Configuration
# =============================================================================
MCP_MSSQL_PATH=/path/to/SQL-AI-samples/MssqlMcp/Node/dist/index.js
MCP_MSSQL_READONLY=false

# =============================================================================
# Application Settings
# =============================================================================
LOG_LEVEL=INFO
```

### ü¶ô LLM Provider Options

| Provider | Recommended Models | Notes |
|----------|-------------------|-------|
| **Ollama** | `qwen2.5:7b-instruct`, `llama3.1:8b` | Requires Ollama running |
| **Foundry Local** | `phi-4`, `phi-3-mini` | Microsoft's local runtime |

---

## üöÄ Running the Agent

### ‚å®Ô∏è CLI Interface

```bash
# Start the CLI chat
uv run python -m src.cli.chat

# With streaming responses
uv run python -m src.cli.chat --stream

# Use Foundry Local instead of Ollama
uv run python -m src.cli.chat --provider foundry_local

# With read-only mode (safer for exploration)
uv run python -m src.cli.chat --readonly

# With debug output
uv run python -m src.cli.chat --debug
```

### üåê Streamlit Web UI

```bash
# Start the web interface
uv run streamlit run src/ui/streamlit_app.py

# Access at: http://localhost:8501
```

> üí° **Tip**: The web UI includes a provider selector in the sidebar to switch between Ollama and Foundry Local.

---

## üß™ Testing the Agent

### Sample Queries to Try

Once the agent is running with the Docker database:

#### üìä Schema Discovery
```
What tables are in the database?
Describe the Researchers table
Show me the schema for Projects
What views are available?
```

#### üîç Basic Queries
```
How many researchers are there?
List all departments and their budgets
Show me the top 5 highest paid researchers
What are the active projects?
```

#### üìà Analytical Queries
```
Which department has the most researchers?
What's the total budget across all projects?
Show me researchers in the AI department
List projects that are over budget
```

#### üîó Relationship Queries
```
How many publications does each researcher have?
What funding sources support the LLM project?
Which researchers are assigned to multiple projects?
```

### ‚úÖ Expected Behavior

| Step | Agent Action |
|------|-------------|
| 1 | Lists tables when asked about database structure |
| 2 | Describes schemas before querying data |
| 3 | Shows results in readable format |
| 4 | Explains its actions as it works |

### üîß Troubleshooting

| Issue | Solution |
|-------|----------|
| Connection refused | Check `docker compose ps` and logs |
| Database does not exist | Re-run `docker compose --profile init up mssql-tools` |
| Ollama connection failed | Run `curl http://localhost:11434/api/tags` |
| MCP server not found | Verify `MCP_MSSQL_PATH` in `.env` |

---

## üîå MCP Tools Reference

| Tool | Description | Mode | Example Use |
|------|-------------|------|-------------|
| `list_tables` | Lists all tables | ‚úÖ Read | "What tables exist?" |
| `describe_table` | Get table schema | ‚úÖ Read | "Describe the Researchers table" |
| `read_data` | Query data | ‚úÖ Read | "Show top 10 researchers" |
| `insert_data` | Insert rows | ‚ö†Ô∏è Write | "Add a new researcher" |
| `update_data` | Modify data | ‚ö†Ô∏è Write | "Update project status" |
| `create_table` | Create tables | ‚ö†Ô∏è Write | "Create an audit log table" |
| `drop_table` | Delete tables | ‚ö†Ô∏è Write | "Remove temp table" |
| `create_index` | Add indexes | ‚ö†Ô∏è Write | "Index the email column" |

> ‚ö†Ô∏è **Warning**: Use `MCP_MSSQL_READONLY=true` to disable write operations.

---

## üèóÔ∏è Architecture

```
+-------------------------------------------------------------+
|                      User Interfaces                         |
|  +------------------+              +----------------------+  |
|  |  ‚å®Ô∏è CLI (Typer)  |              |  üåê Streamlit Web UI |  |
|  +--------+---------+              +-----------+----------+  |
+-----------|------------------------------------|-------------+
            |                                    |
            v                                    v
+-------------------------------------------------------------+
|                    ü§ñ Pydantic AI Agent                      |
|  +-------------------------------------------------------+  |
|  |  System Prompt + Tool Orchestration + Conversation    |  |
|  +-------------------------------------------------------+  |
+----------------------------+--------------------------------+
                             |
            +----------------+----------------+
            v                                 v
+--------------------+       +----------------------------------+
|  ü¶ô LLM Provider   |       |         üîå MCP Servers           |
|  +--------------+  |       |  +----------------------------+ |
|  | Ollama       |  |       |  |    MSSQL MCP Server        | |
|  | Foundry Local|  |       |  |   (SQL Server Access)      | |
|  +--------------+  |       |  +-------------+--------------+ |
+--------------------+       |                |                |
                             |                v                |
                             |  +----------------------------+ |
                             |  |   üóÉÔ∏è SQL Server           | |
                             |  |   (Docker Container)       | |
                             |  |   ResearchAnalytics DB     | |
                             |  +----------------------------+ |
                             +----------------------------------+
```

### üîß Tech Stack

| Component | Technology | Icon |
|-----------|------------|------|
| LLM Runtime | Ollama / Foundry Local | ü¶ô |
| Agent Framework | Pydantic AI | ü§ñ |
| MCP Server | MSSQL MCP (Node.js) | üîå |
| Web UI | Streamlit | üåê |
| CLI | Typer + Rich | ‚å®Ô∏è |
| Database | SQL Server 2022 (Docker) | üóÉÔ∏è |
| Validation | Pydantic v2 | ‚úÖ |

### üìÅ Project Structure

```
local-llm-research-agent/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agent/          # ü§ñ Pydantic AI agent
‚îÇ   ‚îú‚îÄ‚îÄ providers/      # ü¶ô LLM provider abstraction
‚îÇ   ‚îú‚îÄ‚îÄ mcp/            # üîå MCP client and config
‚îÇ   ‚îú‚îÄ‚îÄ cli/            # ‚å®Ô∏è Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ ui/             # üåê Streamlit web interface
‚îÇ   ‚îú‚îÄ‚îÄ models/         # üìã Pydantic data models
‚îÇ   ‚îî‚îÄ‚îÄ utils/          # ‚öôÔ∏è Configuration and logging
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml    # üê≥ SQL Server container
‚îÇ   ‚îî‚îÄ‚îÄ init/                 # üóÉÔ∏è Database init scripts
‚îú‚îÄ‚îÄ tests/              # üß™ Test suite
‚îú‚îÄ‚îÄ docs/               # üìö Documentation
‚îú‚îÄ‚îÄ examples/           # üí° Usage examples
‚îî‚îÄ‚îÄ .github/            # üîß CI/CD workflows
```

---

## üõ†Ô∏è Development

### üß™ Running Tests

```bash
# Run all tests
uv run pytest tests/ -v

# Run only unit tests
uv run pytest tests/ -v -m unit

# Run only integration tests
uv run pytest tests/ -v -m integration

# Run with coverage
uv run pytest tests/ --cov=src --cov-report=html
```

### üìã Code Quality

```bash
# Lint code
uv run ruff check .

# Auto-fix lint issues
uv run ruff check --fix .

# Format code
uv run ruff format .

# Type checking
uv run mypy src/
```

### üîí Pre-commit Hooks

```bash
# Install pre-commit hooks
uv run pre-commit install

# Run hooks manually
uv run pre-commit run --all-files
```

---

## ü§ù Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

> üìå **Important**: All external contributions must be submitted via Pull Request.

### Quick Contribution Steps

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`uv run pytest tests/ -v`)
5. Submit a Pull Request

---

## üîê Security

Please see [SECURITY.md](SECURITY.md) for:
- How to report vulnerabilities
- Security best practices
- Known security considerations

> ‚ö†Ô∏è **Warning**: Never commit credentials or `.env` files!

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- [Pydantic AI](https://ai.pydantic.dev/) - Agent framework
- [Ollama](https://ollama.com/) - Local LLM runtime
- [Microsoft Foundry Local](https://github.com/microsoft/Foundry-Local) - Alternative LLM runtime
- [MSSQL MCP Server](https://github.com/Azure-Samples/SQL-AI-samples/tree/main/MssqlMcp) - SQL Server MCP integration
- [Model Context Protocol](https://modelcontextprotocol.io/) - Tool integration standard

---

*Last Updated: December 2024*
