# Chat Methods Testing Examples

This directory contains test scripts demonstrating the three chat methods:
1. `chat()` - Basic text response with caching
2. `chat_stream()` - Streaming chunks for UX
3. `chat_with_details()` - Full metadata response

## üöÄ Quick Start

### Full Featured Test (Recommended)
```bash
uv run python examples/test_chat_methods.py
```
Beautiful Rich-formatted output showing all three methods with:
- Streaming visualization
- Caching demonstration
- Metadata tables
- Summary comparison

### Quick Test (Minimal)
```bash
uv run python examples/test_chat_quick.py
```
Simple console output without fancy formatting. Good for:
- Quick verification
- Debugging
- CI/CD pipelines

### Individual Method Tests
```bash
# Test just chat()
uv run python examples/test_individual_methods.py chat

# Test just chat_stream()
uv run python examples/test_individual_methods.py stream

# Test just chat_with_details()
uv run python examples/test_individual_methods.py details

# Test all three
uv run python examples/test_individual_methods.py
```

## üìã Prerequisites

1. **Docker containers running:**
   ```bash
   docker-compose -f docker/docker-compose.yml --env-file .env up -d
   ```

2. **Ollama running with model:**
   ```bash
   ollama serve
   ollama pull qwen3:30b
   ```

3. **MCP servers configured:**
   - Check `mcp_config.json` has enabled servers
   - At minimum need `mssql` server for database queries

## üéØ What Each Test Shows

### test_chat_methods.py
- ‚úÖ Session management (one context for all methods)
- ‚úÖ Caching effectiveness (speed comparison)
- ‚úÖ Streaming visualization (typing effect)
- ‚úÖ Metadata extraction (tokens, duration, model)
- ‚úÖ Summary table comparing all three

**Output example:**
```
=======================================================================
Method 1: chat()
Simple text response with caching
=======================================================================

‚Üí Sending: What tables are in the database?
‚úì Response (1250ms):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ The database contains: Researchers, Projects...‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚Üí Sending SAME message again (testing cache):
‚úì Cached response (5ms) - 250x faster!

=======================================================================
Method 2: chat_stream()
AsyncIterator yielding text chunks
=======================================================================

‚Üí Sending: List all researchers
‚úì Streaming response:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Here are the researchers in the database: [chunks appear gradually]
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚úì Streaming complete:
  ‚Ä¢ Duration: 1100ms
  ‚Ä¢ Chunks: 55
  ‚Ä¢ Total length: 1234 chars
  ‚Ä¢ Tokens: 450 (input: 25, output: 425)
```

### test_chat_quick.py
Simple text output - perfect for CI/CD or quick verification:
```
============================================================
Quick Chat Methods Test
============================================================

1. Creating agent...
2. Entering agent context...
   ‚úì MCP sessions established

============================================================
TEST 1: chat() - Basic response
============================================================
Response: The database contains the following tables...
Length: 850 chars

‚úì All tests complete!
```

### test_individual_methods.py
Test methods in isolation - good for debugging specific issues:
```bash
# Example: Debug just streaming
uv run python examples/test_individual_methods.py stream

=== Testing chat_stream() ===

Streaming query results:

------------------------------------------------------------
Department 1: AI Research
Department 2: Data Science
...
------------------------------------------------------------

Chunks received: 45
Stats: {'token_usage': TokenUsage(total=380, input=20, output=360)}

‚úì Test complete
```

## üîç Verifying Session Management

All tests demonstrate proper session management:

```
>>> ENTERING AGENT CONTEXT (MCP sessions established) <<<
‚úì MCP sessions established and ready

[Multiple chat method calls here - all reuse the same session]

>>> EXITING AGENT CONTEXT (MCP sessions closed) <<<
```

**What to look for:**
- ‚úÖ "MCP sessions established" appears ONCE at start
- ‚úÖ No additional session establishment between messages
- ‚úÖ "MCP sessions closed" appears ONCE at end

**Red flags (bug not fixed):**
- ‚ùå Multiple "sessions established" messages
- ‚ùå POST requests on every message (check logs)
- ‚ùå Slow performance between messages

## üêõ Debugging

### Enable Debug Logging
```bash
# In .env
LOG_LEVEL=DEBUG

# Then run tests and grep for POST requests
uv run python examples/test_chat_methods.py 2>&1 | grep "POST"
```

### Check Session IDs
```bash
# Look for Mcp-Session-Id headers in logs
uv run python examples/test_chat_methods.py 2>&1 | grep -i "session-id"

# Should see SAME session ID reused across all messages
```

### Network Inspection
```bash
# Monitor HTTP traffic (requires Wireshark or tcpdump)
# Look for POST to http://localhost:8051/mcp
# Should see: 1 POST at startup, then tool calls with session ID
```

## üìä Performance Expectations

**With proper session management (after fix):**
| Method | First Call | Subsequent Calls |
|--------|------------|------------------|
| `chat()` | 800-1500ms | 1-10ms (cached) |
| `chat_stream()` | 800-1500ms | 800-1500ms (no cache) |
| `chat_with_details()` | 800-1500ms | 800-1500ms (no cache) |

**With broken session management (before fix):**
| Method | Every Call |
|--------|------------|
| All methods | +200-500ms overhead from reconnection |

## üéì Learning Examples

### How to Use chat() in Your Code
```python
from src.agent.core import ResearchAgent

agent = ResearchAgent(mcp_servers=["mssql"])

async with agent:
    # Simple API endpoint
    response = await agent.chat("What tables exist?")
    return {"answer": response}
```

### How to Use chat_stream() in CLI
```python
async with agent:
    # Interactive chat loop
    user_input = input("You: ")
    print("Agent: ", end="", flush=True)
    
    async for chunk in agent.chat_stream(user_input):
        print(chunk, end="", flush=True)
    
    print()  # New line after streaming
```

### How to Use chat_with_details() for Monitoring
```python
async with agent:
    response = await agent.chat_with_details("Complex query")
    
    # Log metrics
    logger.info(
        "agent_query",
        duration_ms=response.duration_ms,
        tokens=response.token_usage.total_tokens,
        success=response.success,
        model=response.model
    )
    
    return response.content
```

## üÜò Troubleshooting

### "Agent not initialized" error
```bash
# Check MCP servers are configured and enabled
cat mcp_config.json | jq '.mcpServers'

# Verify Docker containers running
docker ps | grep mssql
```

### Slow responses
```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Verify model is pulled
ollama list | grep qwen3
```

### No caching in chat()
```bash
# Check cache is enabled in .env
grep CACHE .env

# Verify message is EXACT match (case-sensitive)
# Cache only works for identical messages
```

## üìù Notes

- All tests use **local Ollama** for inference (no external APIs)
- Tests connect to **local SQL Server** in Docker
- First run may be slow (model loading, database connection)
- Caching only works with `chat()` method
- Streaming is simulated (20 char chunks) - not true LLM streaming

## üîó Related Documentation

- Main docs: `../CLAUDE.md`
- Session bug fix: `../MCP_SESSION_BUG_FIX.md`
- Agent source: `../src/agent/core.py`
- CLI source: `../src/cli/chat.py`
